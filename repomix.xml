This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-11-19 12:53:04

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
CODE_OF_CONDUCT.md
LICENSE
CHANGELOG.md
.pre-commit-config.yaml
README.md
.gitignore
CONTRIBUTING.md
.github
  CODEOWNERS
  workflows
    checks.yml
    mcp-server-release.yml
  ISSUE_TEMPLATE
    feature_request.md
    bug_report.md
    custom.md
  dependabot.yml
  SECURITY.md
commitlint.config.js
modelcontextprotocol
  .cursor
    rules
      tool-development-guide.mdc
      python.mdc
      project-structure.mdc
      mcp-guidelines.mdc
  tools
    dsl.py
    models.py
    query.py
    __init__.py
    dq_rules.py
    assets.py
    search.py
    lineage.py
    glossary.py
  version.py
  Dockerfile
  client.py
  pyproject.toml
  utils
    constants.py
    __init__.py
    parameters.py
    search.py
  .dockerignore
  .python-version
  settings.py
  middleware.py
```

# Repository Files


## CODE_OF_CONDUCT.md

```markdown
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to make participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

- Using welcoming and inclusive language
- Being respectful of differing viewpoints and experiences
- Gracefully accepting constructive criticism
- Focusing on what is best for the community
- Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

- The use of sexualized language or imagery and unwelcome sexual attention or
  advances
- Trolling, insulting/derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others' private information, such as a physical or electronic
  address, without explicit permission
- Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies within all project spaces, and it also applies when
an individual is representing the project or its community in public spaces.
Examples of representing a project or community include using an official
project e-mail address, posting via an official social media account, or acting
as an appointed representative at an online or offline event. Representation of
a project may be further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at [hello@atlan.com](mailto:hello@atlan.com). All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq
```

## LICENSE

```text
MIT License

Copyright (c) 2025 Atlan Pte Ltd.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## CHANGELOG.md

```markdown
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.2.11] - 2025-11-07

### Changed
- Upgraded FastMCP dependency from 2.11.0 to 2.13.0.2

## [0.2.10] - 2025-10-13

### Fixed
- Fixed a bug in `update_assets_tool` where glossary terms and categories could not be updated because the glossary GUID parameter was not being sent - now properly includes the glossary GUID as a required parameter for these asset types

## [0.2.9] - 2025-09-22

### Fixed
- Transport configuration not working when installed via PyPI and executed using `uvx atlan-mcp-server` - server would ignore environment variables and command-line arguments, always defaulting to stdio mode

## [0.2.8] - 2025-09-15

### Added
- Term linking functionality for improved glossary term relationships (#138)
- Enhanced search tool with popularity attributes context and examples (#132)
- Comprehensive MCP transport mode documentation (#124)

### Changed
- Implemented client singleton pattern for improved connection pool reuse and performance (#131)
- Enhanced Docker configuration with improved .dockerignore settings (#127)

## [0.2.7] - 2025-09-02

### Added
- Configurable tool access control via `RESTRICTED_TOOLS` environment variable
- Organizations can now restrict any combination of tools for MCP clients


## [0.2.6] - 2025-08-19

### Added
- Glossary management tools to streamline glossary creation and management:
  - `create_glossaries`: Create top-level glossaries with metadata (name, `user_description`, optional `certificate_status`)
  - `create_glossary_terms`: Add individual terms to an existing glossary; supports `user_description`, optional `certificate_status`, and `category_guids`
  - `create_glossary_categories`: Add categories (and nested subcategories) anchored to a glossary or parent category; supports `user_description` and optional `certificate_status`
- Bulk creation support across glossaries, terms, and categories to enable scalable glossary builds
- Foundation for automated, structured glossary generation from unstructured content


## [0.2.5] - 2025-08-05

### Changed
- Enhanced `search_assets_tool` documentation and usage examples for `connection_qualified_name` parameter
- Added `connection_qualified_name` parameter to example function calls for missing descriptions and multiple asset types searches

## [0.2.4] - 2025-07-24

### Added
- Enhanced lineage traversal tool with configurable attribute inclusion support
- `include_attributes` parameter in `traverse_lineage_tool` allowing users to specify additional attributes beyond defaults
- Default attributes are now always included: name, display_name, description, qualified_name, user_description, certificate_status, owner_users, owner_groups, connector_name, has_lineage, source_created_at, source_updated_at, readme, asset_tags

### Changed
- Improved lineage tool return format to standardized dictionary structure with `assets` and `error` keys
- Enhanced lineage processing using Pydantic serialization with `dict(by_alias=True, exclude_unset=True)` for consistent API responses
- Updated `immediate_neighbors` default value from `True` to `False` to align with underlying FluentLineage behavior
- Better error handling and logging throughout lineage traversal operations

### Fixed
- Lineage tool now returns richer attribute information instead of just default minimal attributes
- Resolved issue where lineage results only contained basic metadata without requested additional attributes

## [0.2.3] - 2025-07-16

### Added
- Expanded docstring attributes for LLM context in `server.py` for improved clarity and developer experience

### Changed
- Major documentation and README refactoring for easier setup and integration with Claude Desktop and Cursor, including clearer configuration examples and troubleshooting guidance

### Fixed
- Made `ATLAN_MCP_USER_AGENT` dynamic in `settings.py` to always reflect the current MCP server version in API requests

## [0.2.2] - 2025-06-23

### Added
- Multi-architecture build support for Docker images (ARM64 and AMD64)
- README support for asset updates - allows updating asset documentation/readme using markdown content
- Enhanced parameter parsing utilities for better Claude Desktop integration

### Fixed
- Search and Update Assets Tool compatibility issues with Claude Desktop
- String input parsing from Claude Desktop for better tool interaction
- Parameter validation and error handling improvements

### Changed
- Upgraded FastMCP dependency version for improved performance and stability
- Enhanced tool parameter processing with better error handling
- Improved asset update functionality with support for README content management

## [0.2.1] - 2025-05-24

### Added
- Advanced search operators support in `search_assets` including `contains`, `between`, and case-insensitive comparisons
- Default attributes for search results via `DEFAULT_SEARCH_ATTRIBUTES` constant with dynamic user-specified attribute support
- Enhanced "some conditions" handling with support for advanced operators and case-insensitive logic
- New search examples demonstrating OR logic for multiple type names and glossary term searches by specific attributes

### Changed
- Integrated `SearchUtils` for centralized and consistent search result processing
- Improved search API flexibility and precision with advanced query capabilities

### Fixed
- Release workflow changelog generation issues that previously caused empty release notes
- Improved commit range calculation and error handling in GitHub Actions workflow

## [0.2.0] - 2025-05-17

### Added
- Support for new transport modes: streamable HTTP and SSE
- MCP server executable script (`atlan-mcp-server`)
- Improved Docker image with non-root user and security enhancements

### Changed
- Made MCP server an installable package
- Updated dependencies and bumped versions
- Improved build process for faster Docker builds
- Restructured release workflow for better isolation and PR-based releases

### Fixed
- Various minor bugs and stability issues

### Documentation
- Updated setup and usage instructions
- Added more comprehensive examples


## [0.1.0] - 2024-05-05

### Added
- Initial release of Atlan MCP Server
- Basic functionality for integrating with Atlan
```

## .pre-commit-config.yaml

```yaml
repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
    -   id: trailing-whitespace
    -   id: end-of-file-fixer
    -   id: check-yaml
    -   id: check-added-large-files
    -   id: check-ast
    -   id: check-json
    -   id: check-merge-conflict
    -   id: detect-private-key

-   repo: https://github.com/alessandrojcm/commitlint-pre-commit-hook
    rev: v9.11.0
    hooks:
    -   id: commitlint
        stages: [commit-msg]
        additional_dependencies: ['@commitlint/config-conventional']

-   repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.0
    hooks:
    -   id: ruff
        args: [--fix, --exit-non-zero-on-fix]
    -   id: ruff-format
```

## README.md

```markdown
# Atlan Agent Toolkit

[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md)
[![PyPI - Version](https://img.shields.io/pypi/v/atlan-mcp-server.svg)](https://pypi.org/project/atlan-mcp-server)
[![License](https://img.shields.io/github/license/atlanhq/agent-toolkit.svg)](https://github.com/atlanhq/agent-toolkit/blob/main/LICENSE)


This repository contains a collection of tools and protocols for interacting with Atlan services for AI agents. Each component is designed to provide specific functionality and can be used independently or together.

## Components

### Model Context Protocol (MCP)

An MCP server that enables interaction with Atlan services through tool calling. Provides tools for asset search, and retrieval using [pyatlan](https://developer.atlan.com/sdks/python/).

You can find the documentation and setup instructions for the MCP server [here](modelcontextprotocol/README.md).


## ðŸ” DeepWiki: Ask Questions About This Project

[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/atlanhq/agent-toolkit)


## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to contribute to the Atlan Agent Toolkit.


## License

The project is licensed under the [MIT License](LICENSE). Please see the [LICENSE](LICENSE) file for details.
```

## .gitignore

```text
# Repomix output files
repomix-output.xml
repomix-output.txt

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
```

## CONTRIBUTING.md

````markdown
# Contributing

We welcome contributions to the Atlan Agent Toolkit! Please follow these guidelines when submitting pull requests:

1. **Create a New Branch:**
   - Create a new branch for your changes.
   - Use a descriptive name for the branch (e.g., `feature/add-new-tool`).

2. **Make Your Changes:**
   - Make your changes in the new branch.
   - Ensure your tools are well-defined and follow the MCP specification.

3. **Submit a Pull Request:**
   - Push your changes to your branch.
   - Create a pull request against the `main` branch.
   - Provide a clear description of the changes and any related issues.
   - Ensure the PR passes all CI checks before requesting a review.

4. **Code Quality:**
   - We use pre-commit hooks to maintain code quality.
   - Install pre-commit in your local environment:
     ```bash
     uv pip install pre-commit
     pre-commit install
     ```
   - Pre-commit will automatically run checks before each commit, including:
     - Code formatting with Ruff
     - Trailing whitespace removal
     - End-of-file fixing
     - YAML and JSON validation
     - Other quality checks

5. **Environment Setup:**
   - This project uses [uv](https://docs.astral.sh/uv/) for dependency management.
   - Refer to the [Model Context Protocol README](modelcontextprotocol/README.md) for setup instructions.
   - Python 3.11 or higher is required.

6. **Documentation:**
   - Update documentation to reflect your changes.
   - Add comments to your code where necessary.

Please open an issue or discussion for questions or suggestions before starting significant work!
````

## .github/CODEOWNERS

```text
# This file defines code owners for different parts of the repository
# Code owners are automatically requested for review when someone opens a pull request
# that modifies code that they own.

# Default owners for everything in the repo
*       @Hk669 @firecast

# Model Context Protocol specific files
/modelcontextprotocol/  @Hk669 @firecast
```

## .github/workflows/checks.yml

```yaml
# Pre-commit-checks. This can be reused across all the applications.

name: Pre-commit Checks
on:
  workflow_call:
  pull_request:
    types: [ opened, synchronize, labeled, reopened ]
    branches: "main"

jobs:
  pre-commit:
    concurrency:
      group: ${{ github.workflow }}-${{ github.ref }}
      cancel-in-progress: ${{ startsWith(github.ref, 'refs/pull/') }}
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
    - uses: actions/checkout@v4
    #----------------------------------------------
    #  -----  install & configure Python + UV  -----
    #----------------------------------------------
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    - name: Install UV
      uses: astral-sh/setup-uv@v5
    #----------------------------------------------
    #  -----  install dependencies & run pre-commit  -----
    #----------------------------------------------
    - name: Install dependencies and run pre-commit
      run: |
        uv pip install --system pre-commit
        # Run pre-commit directly
        pre-commit run --all-files
```

## .github/workflows/mcp-server-release.yml

```yaml
name: MCP-Release

on:
  pull_request:
    types: [closed]
    branches:
      - main

jobs:
  prepare-release:
    # Only run when a PR with the "release" label is merged
    if: github.event.pull_request.merged == true && contains(github.event.pull_request.labels.*.name, 'release')
    runs-on: ubuntu-latest
    permissions:
      contents: write
    outputs:
      version: ${{ steps.get_version.outputs.version }}
      should_release: ${{ steps.check_tag.outputs.exists == 'false' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get version
        id: get_version
        run: |
          VERSION=$(grep -m 1 "__version__" modelcontextprotocol/version.py | cut -d'"' -f2)
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Found version: $VERSION"

      - name: Check if tag exists
        id: check_tag
        run: |
          TAG_NAME="v${{ steps.get_version.outputs.version }}"
          if git rev-parse "$TAG_NAME" >/dev/null 2>&1; then
            echo "Tag $TAG_NAME already exists, stopping workflow"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "Tag $TAG_NAME does not exist, continuing workflow"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate changelog entry
        id: changelog
        if: steps.check_tag.outputs.exists == 'false'
        run: |
          set +e

          VERSION="${{ steps.get_version.outputs.version }}"
          RELEASE_DATE=$(date +"%Y-%m-%d")

          echo "Generating changelog for version $VERSION ($RELEASE_DATE)"

          # Get the previous version tag
          PREV_TAG=$(git describe --tags --abbrev=0 HEAD~1 2>/dev/null || echo "")

          if [ -z "$PREV_TAG" ]; then
            # If no previous tag, get the first commit
            FIRST_COMMIT=$(git rev-list --max-parents=0 HEAD)
            RANGE="$FIRST_COMMIT..HEAD"
            echo "Using range from first commit to HEAD"
          else
            RANGE="$PREV_TAG..HEAD"
            echo "Using range from $PREV_TAG to HEAD"
          fi

          # Create temporary changelog entry for RELEASE_NOTES.md
          echo "## [$VERSION] - $RELEASE_DATE" > RELEASE_NOTES.md
          echo "" >> RELEASE_NOTES.md

          # Add features
          git log $RANGE --format="* %s (%h)" --grep="^feat" --perl-regexp --no-merges 2>/dev/null > features.txt || touch features.txt

          if [ -s features.txt ]; then
            echo "### Added" >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
            sed 's/^\* feat[[:space:]]*\([^:]*\):[[:space:]]*/* /' features.txt >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
          fi

          # Add fixes
          git log $RANGE --format="* %s (%h)" --grep="^fix" --perl-regexp --no-merges 2>/dev/null > fixes.txt || touch fixes.txt

          if [ -s fixes.txt ]; then
            echo "### Fixed" >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
            sed 's/^\* fix[[:space:]]*\([^:]*\):[[:space:]]*/* /' fixes.txt >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
          fi

          # Add other changes (excluding merge commits, chore, docs, style, refactor, test, ci)
          git log $RANGE --format="* %s (%h)" --no-merges 2>/dev/null | \
            grep -v -E "^\* (feat|fix|chore|docs|style|refactor|test|ci)(\(.*\))?:" > others.txt || touch others.txt

          if [ -s others.txt ]; then
            echo "### Changed" >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
            cat others.txt >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
          fi

          # If no specific changes found, add a simple entry
          if [ ! -s features.txt ] && [ ! -s fixes.txt ] && [ ! -s others.txt ]; then
            echo "### Changed" >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
            echo "* Release version $VERSION" >> RELEASE_NOTES.md
            echo "" >> RELEASE_NOTES.md
          fi

          # Clean up temporary files
          rm -f features.txt fixes.txt others.txt

          echo "Release notes generated successfully"
          echo "================================"
          cat RELEASE_NOTES.md
          echo "================================"

      - name: Create Tag
        if: steps.check_tag.outputs.exists == 'false'
        run: |
          git tag v${{ steps.get_version.outputs.version }}
          git push --tags

      - name: Create GitHub Release
        if: steps.check_tag.outputs.exists == 'false'
        uses: softprops/action-gh-release@v2
        with:
          tag_name: v${{ steps.get_version.outputs.version }}
          body_path: RELEASE_NOTES.md
          token: ${{ secrets.GITHUB_TOKEN }}
          draft: false
          prerelease: false

      # Upload release notes for other jobs to use
      - name: Upload release notes
        if: steps.check_tag.outputs.exists == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: release-notes
          path: RELEASE_NOTES.md
          retention-days: 1

  publish-pypi:
    needs: prepare-release
    if: needs.prepare-release.outputs.should_release == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: v${{ needs.prepare-release.outputs.version }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build wheel twine

      - name: Build package
        run: |
          cd modelcontextprotocol
          python -m build

      - name: Publish to PyPI
        env:
          TWINE_USERNAME: ${{ secrets.PYPI_USERNAME }}
          TWINE_PASSWORD: ${{ secrets.PYPI_PASSWORD }}
        run: |
          cd modelcontextprotocol
          twine upload dist/*

  publish-docker:
    needs: prepare-release
    if: needs.prepare-release.outputs.should_release == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: v${{ needs.prepare-release.outputs.version }}

      - name: Set up QEMU for Cross-Platform Builds
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: ./modelcontextprotocol/
          push: true
          tags: |
            ghcr.io/atlanhq/atlan-mcp-server:latest
            ghcr.io/atlanhq/atlan-mcp-server:${{ needs.prepare-release.outputs.version }}
          platforms: |
            linux/amd64
            linux/arm64
```

## .github/ISSUE_TEMPLATE/feature_request.md

```markdown
---
name: Feature request
about: Suggest a new feature or enhancement for the agent toolkit
title: '[FEATURE] '
labels: 'enhancement'
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Technical Details**
- Proposed API changes (if any)
- Impact on existing functionality
- Required dependencies or new packages

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context about the feature request here, such as:
- Use cases or scenarios
- Related issues or pull requests
- Implementation considerations
```

## .github/ISSUE_TEMPLATE/bug_report.md

````markdown
---
name: Bug report
about: Report a bug or unexpected behavior in the agent toolkit
title: '[BUG] '
labels: 'bug'
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Python version and environment details
2. Command or code that triggered the bug
3. Expected output vs actual output

**Environment Information**
- Python version: [e.g. 3.9.0]
- OS: [e.g. macOS, Linux, Windows]
- Package versions: [e.g. python-dotenv==1.0.0, pydantic==2.0.0]

**Error Message**
```
Paste any error messages or stack traces here
```

**Expected behavior**
A clear and concise description of what you expected to happen.

**Additional context**
Add any other context about the problem here, such as:
- Related configuration files
- Relevant environment variables
- Any workarounds you've tried
````

## .github/ISSUE_TEMPLATE/custom.md

```markdown
---
name: Documentation or Question
about: Ask a question or request documentation improvements
title: '[DOCS] '
labels: 'documentation'
assignees: ''

---

**What is your question or documentation request?**
A clear and concise description of what you need help with or what documentation you'd like to see improved.

**Additional context**
Add any other context about your question or documentation request here, such as:
- Related documentation you've already reviewed
- Specific sections that need clarification
- Examples or use cases you'd like to see documented
```

## .github/dependabot.yml

```yaml
version: 2
updates:
  - package-ecosystem: pip
    directory: "/modelcontextprotocol"
    schedule:
      interval: daily
    open-pull-requests-limit: 100
    allow:
      - dependency-type: "all"

  - package-ecosystem: "github-actions"
    directory: "/modelcontextprotocol"
    schedule:
      interval: daily
```

## .github/SECURITY.md

```markdown
# Vulnerability Disclosure

If you think you have found a potential security vulnerability,
please open a [draft Security Advisory](https://github.com/atlanhq/agent-toolkit/security/advisories/new)
via GitHub. We will coordinate verification and next steps through
that secure medium.

If English is not your first language, please try to describe the
problem and its impact to the best of your ability. For greater detail,
please use your native language and we will try our best to translate it
using online services.

Please also include the code you used to find the problem and the
shortest amount of code necessary to reproduce it.

Please do not disclose this to anyone else. We will retrieve a CVE
identifier if necessary and give you full credit under whatever name or
alias you provide. We will only request an identifier when we have a fix
and can publish it in a release.

We will respect your privacy and will only publicize your involvement if
you grant us permission.
```

## commitlint.config.js

```javascript
module.exports = {
  extends: ['@commitlint/config-conventional'],
  rules: {
    'type-enum': [
      2,
      'always',
      [
        'feat',
        'fix',
        'docs',
        'style',
        'refactor',
        'perf',
        'test',
        'build',
        'ci',
        'chore',
        'revert'
      ]
    ],
    'subject-case': [0], // Disabled to allow any case
  }
};
```

## modelcontextprotocol/.cursor/rules/tool-development-guide.mdc

````text
---
description:
globs:
alwaysApply: true
---
This guide outlines the core coding patterns for implementing tools in the Atlan MCP server.

## Tool Implementation Pattern

1. Define core function in tools.py
2. Register function as MCP tool in server.py
3. Use appropriate type hints for PyAtlan and MCP compatibility

## Core Function Template (tools.py)

```python
def implement_atlan_tool(
    param1: str,
    param2: Optional[int] = None,
    param3: Optional[Dict[str, Any]] = None
) -> Any:
    """
    Implement an Atlan tool with appropriate parameters.
    """
    logger.info(f"Starting tool execution with parameters: param1={param1}")

    try:
        # Tool-specific implementation
        result = execute_atlan_operation(param1, param2, param3)

        logger.info(f"Tool execution completed successfully")
        return result
    except Exception as e:
        logger.error(f"Error executing tool: {str(e)}")
        logger.exception("Exception details:")
        return get_appropriate_default_value()
```

## MCP Tool Registration Template (server.py)

```python
@mcp.tool()
def registered_tool_name(
    param1: str,
    param2: Optional[int] = None,
    param3: Optional[Dict[str, Any]] = None
) -> Any:
    """
    Tool description with clear purpose.

    Args:
        param1: First parameter description
        param2: Second parameter description
        param3: Third parameter description (complex structure)

    Returns:
        Description of return value

    Example:
        registered_tool_name("value1", 42, {"key": "value"})
    """
    return implement_atlan_tool(param1, param2, param3)
```

## Common Operation Patterns

### Asset Search Operation
```python
def search_operation(criteria):
    search = FluentSearch()
    # Add filters based on criteria
    request = search.to_request()
    results = list(atlan_client.asset.search(request).current_page())
    return results
```

### Asset Retrieval Operation
```python
def get_asset_operation(qualified_name, asset_type):
    try:
        asset = asset_type.get_by_qualified_name(
            qualified_name=qualified_name,
            min_ext_info=True,
            atlan_client=atlan_client
        )
        return asset
    except Exception as e:
        logger.error(f"Failed to get asset: {e}")
        return None
```

### Batch Processing Operation
```python
def batch_operation(items, process_function):
    results = []
    for item in items:
        try:
            result = process_function(item)
            results.append(result)
        except Exception as e:
            logger.warning(f"Error processing item {item}: {e}")
            results.append(None)
    return results
```

### DSL Query Operation
```python
def dsl_query_operation(dsl_query):
    try:
        dsl_dict = json.loads(dsl_query)
        index_request = IndexSearchRequest(dsl=DSL(**dsl_dict))
        results = atlan_client.asset.search(index_request)
        return results
    except Exception as e:
        logger.error(f"DSL query error: {e}")
        return None
```

## Parameter Validation Patterns

```python
def validate_parameters(param, expected_type, allowed_values=None):
    if param is None:
        return False

    if not isinstance(param, expected_type):
        return False

    if allowed_values and param not in allowed_values:
        return False

    return True
```

## Result Formatting Patterns

```python
def format_asset_results(assets):
    return [
        {
            "name": asset.name,
            "qualified_name": asset.qualified_name,
            "type": asset.type_name,
            "description": asset.description
        }
        for asset in assets if asset
    ]
```

## Error Handling Pattern

```python
try:
    # Attempt operation
    result = perform_operation()
    return result
except ValueError as e:
    logger.error(f"Invalid input parameter: {str(e)}")
    return {"error": "Invalid input", "details": str(e)}
except ConnectionError as e:
    logger.error(f"Connection to Atlan failed: {str(e)}")
    return {"error": "Connection failed", "details": str(e)}
except Exception as e:
    logger.error(f"Operation failed: {str(e)}")
    logger.exception("Exception details:")
    return {"error": "Unknown error", "details": str(e)}
```

## Common Type Patterns

```python
from typing import Optional, Dict, Any, List, Union, Type, TypeVar, Callable, Tuple

# Generic result type
Result = Union[Dict[str, Any], List[Dict[str, Any]], None]

# Function returning success status and result
def operation_with_status() -> Tuple[bool, Optional[Result], Optional[str]]:
    try:
        result = perform_operation()
        return True, result, None
    except Exception as e:
        return False, None, str(e)
```

## Logging Pattern

```python
# Start of operation
logger.info(f"Starting {operation_name} with {parameters}")

# Debug information during operation
logger.debug(f"Intermediate state: {some_variable}")

# Operation completed
logger.info(f"Operation {operation_name} completed with {result_summary}")

# Error handling
logger.error(f"Operation {operation_name} failed: {error_message}")
logger.exception("Exception details:")
```
````

## modelcontextprotocol/.cursor/rules/python.mdc

```text
---
description:
globs:
alwaysApply: true
---
You are an AI assistant specialized in Python development. Your approach emphasizes:

Clear project structure with separate directories for source code, docs, and config.

Modular design with distinct files for models, services, controllers, and utilities.

Configuration management using environment variables.

Robust error handling and logging, including context capture.

Detailed documentation using docstrings and README files.

Dependency management via https://github.com/astral-sh/uv and virtual environments.

Code style consistency using Ruff.

CI/CD implementation with GitHub Actions or GitLab CI.

AI-friendly coding practices:

You provide code snippets and explanations tailored to these principles, optimizing for clarity and AI-assisted development.

Follow the following rules:

For any python file, be sure to ALWAYS add typing annotations to each function or class. Be sure to include return types when necessary. Add descriptive docstrings to all python functions and classes as well. Please use pep257 convention. Update existing docstrings if need be.

Make sure you keep any comments that exist in a file.
```

## modelcontextprotocol/.cursor/rules/project-structure.mdc

````text
---
description:
globs:
alwaysApply: true
---
# Atlan MCP Server Project Structure

This document outlines the recommended project structure for the Atlan MCP server.

## Directory Structure

```
modelcontextprotocol/
â”œâ”€â”€ .cursor/                  # Cursor IDE specific files
â”œâ”€â”€ .gitignore                # Git ignore file (Assumed)
â”œâ”€â”€ .python-version           # Python version specification
â”œâ”€â”€ client.py                 # Atlan client factory
â”œâ”€â”€ pyproject.toml            # Project metadata and dependencies
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ server.py                 # MCP server entry point
â”œâ”€â”€ settings.py               # Application settings
â”œâ”€â”€ tools.py                  # Main tool registration/definition file
â”œâ”€â”€ uv.lock                   # uv lock file
â””â”€â”€ tools/                    # Directory for tool implementations/modules
```

## Key Components

### server.py
The main entry point for the MCP server. Registers and exposes tools defined in the `tools/` directory to interact with Atlan.

```python
from mcp.server.fastmcp import FastMCP
from tools import (
    search_assets,
    get_assets_by_dsl,
    traverse_lineage,
    update_assets,
    UpdatableAttribute,
    CertificateStatus,
    UpdatableAsset,
)
from pyatlan.model.fields.atlan_fields import AtlanField
from typing import Optional, Dict, Any, List, Union, Type
from pyatlan.model.assets import Asset
from pyatlan.model.lineage import LineageDirection

mcp = FastMCP("Atlan MCP", dependencies=["pyatlan"])

# Note: Docstrings and full parameter lists omitted for brevity.
# Refer to the actual server.py and tools implementations for details.

@mcp.tool()
def search_assets_tool(
    conditions: Optional[Union[Dict[str, Any], str]] = None,
    # ... many other parameters ...
):
    """Advanced asset search using FluentSearch with flexible conditions."""
    return search_assets(conditions=conditions, ...)

@mcp.tool()
def get_assets_by_dsl_tool(dsl_query: Union[str, Dict[str, Any]]):
    """Execute the search with the given DSL query."""
    return get_assets_by_dsl(dsl_query)

@mcp.tool()
def traverse_lineage_tool(
    guid: str,
    direction: str, # UPSTREAM or DOWNSTREAM
    # ... other parameters ...
):
    """Traverse asset lineage in specified direction."""
    return traverse_lineage(guid=guid, direction=direction, ...)

@mcp.tool()
def update_assets_tool(
    assets: Union[UpdatableAsset, List[UpdatableAsset]],
    attribute_name: UpdatableAttribute,
    attribute_values: List[Union[CertificateStatus, str]],
):
    """Update one or multiple assets with different values for the same attribute."""
    return update_assets(assets=assets, attribute_name=attribute_name, attribute_values=attribute_values)
```

### settings.py
Configuration settings using Pydantic, loaded from environment variables or a `.env` file.

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    """Application settings loaded from environment variables or .env file."""
    ATLAN_BASE_URL: str
    ATLAN_API_KEY: str
    ATLAN_AGENT_ID: str
    ATLAN_AGENT: str = "atlan-mcp"

    @property
    def headers(self) -> dict:
        """Get the headers for API requests."""
        return {
            "x-atlan-agent": self.ATLAN_AGENT,
            "x-atlan-agent-id": self.ATLAN_AGENT_ID,
            "x-atlan-client-origin": self.ATLAN_AGENT,
        }

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        extra = "allow"
        case_sensitive = False
```

### client.py
Factory for creating and configuring the Atlan client using application settings.

```python
import logging
from pyatlan.client.atlan import AtlanClient
from settings import get_settings

logger = logging.getLogger(__name__)

def get_atlan_client() -> AtlanClient:
    """Create an Atlan client instance using settings loaded from environment."""
    settings = get_settings()
    try:
        client = AtlanClient(
            base_url=settings.ATLAN_BASE_URL, api_key=settings.ATLAN_API_KEY
        )
        client.update_headers(settings.headers)
        logger.info("Atlan client created successfully")
        return client
    except Exception as e:
        logger.error(f"Error creating Atlan client: {e}")
        raise Exception(f"Error creating Atlan client: {e}")

_settings: Optional[Settings] = None


def get_settings() -> Settings:
    """
    Get the singleton Settings instance.
    Loads settings once from environment/file and reuses the instance.

    Returns:
        Settings: The singleton settings instance
    """
    global _settings
    if _settings is None:
        _settings = Settings()
    return _settings
```

### tools/ Directory
Contains the implementation of the different tools exposed by the server.
- `assets.py`: Implements asset-related tools like search and update.
- `dsl.py`: Implements the DSL query tool.
- `lineage.py`: (Potentially) Implements lineage traversal tools.
- `__init__.py`: Makes the directory a package and exports the tool functions.
````

## modelcontextprotocol/.cursor/rules/mcp-guidelines.mdc

```text
---
description:
globs:
alwaysApply: false
---
You are an expert in Python, Model Context Protocol (MCP), and Atlan integration.

Key Principles
- Write concise, technical responses with accurate Python examples for Atlan integration.
- Use modular, well-structured code with clear separation of concerns.
- Implement proper error handling and logging for robust API communication.
- Use descriptive variable names that reflect domain terminology.
- Follow Atlan and Model Context Protocol best practices.
- Create reusable utility functions for common operations.

Python/MCP/Atlan Guidelines
- Use FastMCP for server implementation with clear tool definitions.
- Leverage PyAtlan's rich API for communicating with Atlan services.
- Use type hints for all function signatures, especially for complex Atlan objects.
- Implement proper error handling with appropriate logging.
- Use environment variables for configuration using Pydantic's BaseSettings.
- Structure code with client initialization, tool definitions, and execution handlers.

Code Structure
- Separate server configuration from tool implementations.
- Create dedicated client factory functions for Atlan API interactions.
- Implement consistent logging throughout the application.
- Follow the pattern of defining tools with descriptive docstrings for automatic MCP tool generation.
- Use type annotations compatible with both PyAtlan and MCP.

Atlan-Specific Guidelines
- Use PyAtlan classes ( eg Asset, Table, Column) for type checking and accessing model attributes.
- Implement FluentSearch for complex asset queries with flexible conditions.
- Use CompoundQuery for common filter patterns like active assets.
- Support pagination for large result sets.
- Handle proper error cases for Atlan API responses.
- Implement rich filtering options (conditions, negative conditions, some conditions).
- Support inclusion of specific attributes in results.

Error Handling and Validation
- Log all API requests and responses at appropriate levels.
- Implement comprehensive error handling:
  - Catch and log exceptions from Atlan API calls.
  - Return empty lists or default values for failed operations.
  - Use descriptive error messages with contextual information.
  - Include troubleshooting information in logs.
- Validate input parameters before constructing API requests.

Execution Model
- Use synchronous operations for Atlan API calls.
- Implement appropriate request timeout settings.
- Apply rate limiting for bulk operations.
- Use pagination for large result sets.
- Return well-structured responses that MCP can properly format.

Search Optimization
- Build search queries incrementally with appropriate logging.
- Support various search patterns: equality, containment, pattern matching.
- Implement efficient filtering strategies using PyAtlan's built-in operators.
- Support a wide range of search conditions including date ranges.
- Optimize complex queries with proper indexing strategies.

DSL Query Handling
- Validate and parse DSL JSON properly.
- Provide examples for common DSL patterns.
- Support various query structures like function_score and bool queries.
- Return both results and aggregations.
- Implement proper error handling for malformed DSL queries.

Tools and Utility Functions
- Create helper functions for common operations like:
  - Query building
  - Result formatting
  - Error handling
  - Attribute resolution
- Implement consistent patterns for handling Atlan's typed attributes.

Key Conventions
1. Use PyAtlan's fluent interface for building search queries.
2. Properly handle authentication and API errors.
3. Implement appropriate logging at multiple levels:
   - Debug for query construction details
   - Info for operation completion
   - Warning for non-critical issues
   - Error for failures
4. Follow MCP conventions for tool registration and execution.
5. Provide rich documentation for tools with examples.

Dependencies
- mcp[cli] for MCP server implementation
- pyatlan for Atlan API integration
- pydantic-settings for configuration management
- logging for application logging
```

## modelcontextprotocol/tools/dsl.py

```python
import logging
import json
from typing import Dict, Any, Union

from client import get_atlan_client
from pyatlan.model.search import DSL, IndexSearchRequest
from utils.search import SearchUtils

# Configure logging
logger = logging.getLogger(__name__)


def get_assets_by_dsl(dsl_query: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """
    Execute the search with the given query
    Args:
        dsl_query (Union[str, Dict[str, Any]]): The DSL query as either a string or dictionary
    Returns:
        Dict[str, Any]: A dictionary containing the results and aggregations
    """
    logger.info("Starting DSL-based asset search")
    try:
        # Parse string to dict if needed
        if isinstance(dsl_query, str):
            logger.debug("Converting DSL string to JSON")
            try:
                dsl_dict = json.loads(dsl_query)
            except json.JSONDecodeError as e:
                logger.error(f"Invalid JSON in DSL query: {e}")
                return {
                    "results": [],
                    "aggregations": {},
                    "error": "Invalid JSON in DSL query",
                }
        else:
            logger.debug("Using provided DSL dictionary")
            dsl_dict = dsl_query

        logger.debug("Creating IndexSearchRequest")
        index_request = IndexSearchRequest(
            dsl=DSL(**dsl_dict),
            suppress_logs=True,
            show_search_score=True,
            exclude_meanings=False,
            exclude_atlan_tags=False,
        )

        logger.info("Executing DSL search request")
        client = get_atlan_client()
        search_response = client.asset.search(index_request)
        processed_results = SearchUtils.process_results(search_response)
        return processed_results
    except Exception as e:
        logger.error(f"Error in DSL search: {str(e)}")
        return {"results": [], "aggregations": {}, "error": str(e)}
```

## modelcontextprotocol/tools/models.py

```python
from enum import Enum
from typing import Optional, List, Union, Dict, Any

from pydantic import BaseModel


class CertificateStatus(str, Enum):
    """Enum for allowed certificate status values."""

    VERIFIED = "VERIFIED"
    DRAFT = "DRAFT"
    DEPRECATED = "DEPRECATED"


class UpdatableAttribute(str, Enum):
    """Enum for attributes that can be updated."""

    USER_DESCRIPTION = "user_description"
    CERTIFICATE_STATUS = "certificate_status"
    README = "readme"
    TERM = "term"


class TermOperation(str, Enum):
    """Enum for term operations on assets."""

    APPEND = "append"
    REPLACE = "replace"
    REMOVE = "remove"


class TermOperations(BaseModel):
    """Model for term operations on assets."""

    operation: TermOperation
    term_guids: List[str]


class UpdatableAsset(BaseModel):
    """Class representing an asset that can be updated."""

    guid: str
    name: str
    qualified_name: str
    type_name: str
    user_description: Optional[str] = None
    certificate_status: Optional[CertificateStatus] = None
    glossary_guid: Optional[str] = None


class Glossary(BaseModel):
    """Payload model for creating a glossary asset."""

    name: str
    user_description: Optional[str] = None
    certificate_status: Optional[CertificateStatus] = None


class GlossaryCategory(BaseModel):
    """Payload model for creating a glossary category asset."""

    name: str
    glossary_guid: str
    user_description: Optional[str] = None
    certificate_status: Optional[CertificateStatus] = None
    parent_category_guid: Optional[str] = None


class GlossaryTerm(BaseModel):
    """Payload model for creating a glossary term asset."""

    name: str
    glossary_guid: str
    user_description: Optional[str] = None
    certificate_status: Optional[CertificateStatus] = None
    category_guids: Optional[List[str]] = None


class DQRuleType(str, Enum):
    """Enum for supported data quality rule types."""

    # Completeness checks
    NULL_COUNT = "Null Count"
    NULL_PERCENTAGE = "Null Percentage"
    BLANK_COUNT = "Blank Count"
    BLANK_PERCENTAGE = "Blank Percentage"

    # Statistical checks
    MIN_VALUE = "Min Value"
    MAX_VALUE = "Max Value"
    AVERAGE = "Average"
    STANDARD_DEVIATION = "Standard Deviation"

    # Uniqueness checks
    UNIQUE_COUNT = "Unique Count"
    DUPLICATE_COUNT = "Duplicate Count"

    # Validity checks
    REGEX = "Regex"
    STRING_LENGTH = "String Length"
    VALID_VALUES = "Valid Values"

    # Timeliness checks
    FRESHNESS = "Freshness"

    # Volume checks
    ROW_COUNT = "Row Count"

    # Custom checks
    CUSTOM_SQL = "Custom SQL"


class DQRuleSpecification(BaseModel):
    """
    Comprehensive model for creating any type of data quality rule.

    Different rule types require different fields:
    - Column-level rules: require column_qualified_name
    - Table-level rules: only require asset_qualified_name
    - Custom SQL rules: require custom_sql, rule_name, dimension
    - Rules with conditions: require rule_conditions (String Length, Regex, Valid Values)
    """

    # Core identification
    rule_type: DQRuleType
    asset_qualified_name: str

    # Column-level specific (required for most rule types except Row Count and Custom SQL)
    column_qualified_name: Optional[str] = None

    # Threshold configuration
    threshold_value: Optional[Union[int, float]] = None
    threshold_compare_operator: Optional[str] = None  # "EQUAL", "GREATER_THAN", etc.
    threshold_unit: Optional[str] = None  # "DAYS", "HOURS", "MINUTES"

    # Alert configuration
    alert_priority: Optional[str] = "NORMAL"  # "LOW", "NORMAL", "URGENT"

    # Custom SQL specific
    custom_sql: Optional[str] = None
    rule_name: Optional[str] = None
    dimension: Optional[str] = None  # "COMPLETENESS", "VALIDITY", etc.

    # Advanced configuration
    rule_conditions: Optional[List[Dict[str, Any]]] = None
    row_scope_filtering_enabled: Optional[bool] = False
    description: Optional[str] = None
```

## modelcontextprotocol/tools/query.py

```python
"""
Query tool for executing SQL queries on table/view assets.

This module provides functionality to execute SQL queries on data sources
using the Atlan client.
"""

import logging
from typing import Dict, Any, Optional

from client import get_atlan_client
from pyatlan.model.query import QueryRequest

# Configure logging
logger = logging.getLogger(__name__)


def query_asset(
    sql: str,
    connection_qualified_name: str,
    default_schema: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Execute a SQL query on a table/view asset.

    Note:
        Use read-only queries to retrieve data.
        Please add reasonable LIMIT clauses to your SQL queries to avoid
        overwhelming the client or causing timeouts. Large result sets can
        cause performance issues or crash the client application.

    Args:
        sql (str): The SQL query to execute (read-only queries)
        connection_qualified_name (str): Connection qualified name to use for the query
            (e.g., "default/snowflake/1705755637")
        default_schema (str, optional): Default schema name to use for unqualified
            objects in the SQL, in the form "DB.SCHEMA"
            (e.g., "RAW.WIDEWORLDIMPORTERS_WAREHOUSE")

    Returns:
        Dict[str, Any]: Dictionary containing:
            - success: Boolean indicating if the query was successful
            - data: Query result data (rows, columns) if successful
            - error: Error message if query failed
            - query_info: Additional query execution information

    Raises:
        Exception: If there's an error executing the query
    """
    logger.info(
        f"Starting SQL query execution on connection: {connection_qualified_name}"
    )
    logger.debug(f"SQL query: {sql}")
    logger.debug(f"Parameters - default_schema: {default_schema}")

    try:
        # Validate required parameters
        if not sql or not sql.strip():
            error_msg = "SQL query cannot be empty"
            logger.error(error_msg)
            return {
                "success": False,
                "data": None,
                "error": error_msg,
                "query_info": {},
            }

        if not connection_qualified_name or not connection_qualified_name.strip():
            error_msg = "Connection qualified name cannot be empty"
            logger.error(error_msg)
            return {
                "success": False,
                "data": None,
                "error": error_msg,
                "query_info": {},
            }

        # Get Atlan client
        logger.debug("Getting Atlan client")
        client = get_atlan_client()

        # Build query request
        logger.debug("Building QueryRequest object")
        query_request = QueryRequest(
            sql=sql,
            data_source_name=connection_qualified_name,
            default_schema=default_schema,
        )

        # Execute query
        logger.info("Executing SQL query")
        query_response = client.queries.stream(request=query_request)

        logger.info("Query executed successfully, returning response")

        return {
            "success": True,
            "data": query_response,
            "error": None,
            "query_info": {
                "data_source": connection_qualified_name,
                "default_schema": default_schema,
                "sql": sql,
            },
        }

    except Exception as e:
        error_msg = f"Error executing SQL query: {str(e)}"
        logger.error(error_msg)
        logger.exception("Exception details:")

        return {
            "success": False,
            "data": None,
            "error": error_msg,
            "query_info": {
                "data_source": connection_qualified_name,
                "default_schema": default_schema,
                "sql": sql,
            },
        }
```

## modelcontextprotocol/tools/__init__.py

```python
from .search import search_assets
from .dsl import get_assets_by_dsl
from .lineage import traverse_lineage
from .assets import update_assets
from .query import query_asset
from .dq_rules import create_dq_rules
from .glossary import (
    create_glossary_category_assets,
    create_glossary_assets,
    create_glossary_term_assets,
)
from .models import (
    CertificateStatus,
    UpdatableAttribute,
    UpdatableAsset,
    TermOperations,
    Glossary,
    GlossaryCategory,
    GlossaryTerm,
    DQRuleType,
    DQRuleSpecification,
)

__all__ = [
    "search_assets",
    "get_assets_by_dsl",
    "traverse_lineage",
    "update_assets",
    "query_asset",
    "create_glossary_category_assets",
    "create_glossary_assets",
    "create_glossary_term_assets",
    "CertificateStatus",
    "UpdatableAttribute",
    "UpdatableAsset",
    "TermOperations",
    "Glossary",
    "GlossaryCategory",
    "GlossaryTerm",
    "create_dq_rules",
    "DQRuleType",
    "DQRuleSpecification",
]
```

## modelcontextprotocol/tools/dq_rules.py

```python
"""
Data Quality Rules creation tool for Atlan MCP server.

This module provides functionality to create data quality rules in Atlan,
supporting column-level, table-level, and custom SQL rules.
"""

from __future__ import annotations
import logging
from typing import Dict, Any, List, Union

from pyatlan.model.assets import DataQualityRule, Table, Column
from pyatlan.model.enums import (
    DataQualityRuleAlertPriority,
    DataQualityRuleThresholdCompareOperator,
    DataQualityDimension,
    DataQualityRuleThresholdUnit,
    DataQualityRuleTemplateConfigRuleConditions,
)
from pyatlan.model.dq_rule_conditions import DQRuleConditionsBuilder

from client import get_atlan_client
from .models import (
    DQRuleSpecification,
    DQRuleType,
)

logger = logging.getLogger(__name__)

# Rule types that require column_qualified_name
COLUMN_LEVEL_RULES = {
    DQRuleType.NULL_COUNT,
    DQRuleType.NULL_PERCENTAGE,
    DQRuleType.BLANK_COUNT,
    DQRuleType.BLANK_PERCENTAGE,
    DQRuleType.MIN_VALUE,
    DQRuleType.MAX_VALUE,
    DQRuleType.AVERAGE,
    DQRuleType.STANDARD_DEVIATION,
    DQRuleType.UNIQUE_COUNT,
    DQRuleType.DUPLICATE_COUNT,
    DQRuleType.REGEX,
    DQRuleType.STRING_LENGTH,
    DQRuleType.VALID_VALUES,
    DQRuleType.FRESHNESS,
}

# Rule types that work at table level
TABLE_LEVEL_RULES = {
    DQRuleType.ROW_COUNT,
}

# Rule types that support conditions
CONDITIONAL_RULES = {
    DQRuleType.STRING_LENGTH,
    DQRuleType.REGEX,
    DQRuleType.VALID_VALUES,
}


def create_dq_rules(
    rules: Union[Dict[str, Any], List[Dict[str, Any]]],
) -> Dict[str, Any]:
    """
    Create one or multiple data quality rules in Atlan.

    Args:
        rules (Union[Dict[str, Any], List[Dict[str, Any]]]): Either a single rule
            specification or a list of rule specifications.

    Returns:
        Dict[str, Any]: Dictionary containing:
            - created_count: Number of rules successfully created
            - created_rules: List of created rule details (guid, qualified_name, rule_type)
            - errors: List of any errors encountered

    Raises:
        Exception: If there's an error creating the rules.
    """
    # Convert single rule to list for consistent handling
    data = rules if isinstance(rules, list) else [rules]
    logger.info(f"Creating {len(data)} data quality rule(s)")
    logger.debug(f"Rule specifications: {data}")

    result = {"created_count": 0, "created_rules": [], "errors": []}

    try:
        # Validate and parse specifications
        specs = []
        for idx, item in enumerate(data):
            try:
                spec = DQRuleSpecification(**item)
                validation_errors = _validate_rule_specification(spec)
                if validation_errors:
                    for error in validation_errors:
                        result["errors"].append(f"Rule {idx + 1}: {error}")
                    continue
                specs.append(spec)
            except Exception as e:
                result["errors"].append(f"Rule {idx + 1} validation error: {str(e)}")
                logger.error(f"Error parsing rule specification {idx + 1}: {e}")

        if not specs:
            logger.warning("No valid rule specifications to create")
            return result

        # Get Atlan client
        client = get_atlan_client()

        # Create rules
        created_assets = []
        for spec in specs:
            try:
                logger.debug(
                    f"Creating {spec.rule_type.value} rule for {spec.asset_qualified_name}"
                )

                # Route to appropriate creator based on rule type
                if spec.rule_type == DQRuleType.CUSTOM_SQL:
                    rule = _create_custom_sql_rule(spec, client)
                elif spec.rule_type in TABLE_LEVEL_RULES:
                    rule = _create_table_level_rule(spec, client)
                elif spec.rule_type in COLUMN_LEVEL_RULES:
                    rule = _create_column_level_rule(spec, client)
                else:
                    result["errors"].append(
                        f"Unsupported rule type: {spec.rule_type.value}"
                    )
                    continue

                created_assets.append(rule)

            except Exception as e:
                error_msg = f"Error creating {spec.rule_type.value} rule: {str(e)}"
                result["errors"].append(error_msg)
                logger.error(error_msg)

        # Bulk save all created rules
        if created_assets:
            logger.info(f"Saving {len(created_assets)} data quality rules")
            response = client.asset.save(created_assets)

            # Process response
            for created_rule in response.mutated_entities.CREATE:
                result["created_rules"].append(
                    {
                        "guid": created_rule.guid,
                        "qualified_name": created_rule.qualified_name,
                        "rule_type": created_rule.dq_rule_type
                        if hasattr(created_rule, "dq_rule_type")
                        else None,
                    }
                )

            result["created_count"] = len(result["created_rules"])
            logger.info(
                f"Successfully created {result['created_count']} data quality rules"
            )

        return result

    except Exception as e:
        error_msg = f"Error in bulk rule creation: {str(e)}"
        logger.error(error_msg)
        result["errors"].append(error_msg)
        return result


def _validate_rule_specification(spec: DQRuleSpecification) -> List[str]:
    """
    Validate a rule specification based on rule type requirements.

    Args:
        spec (DQRuleSpecification): The rule specification to validate

    Returns:
        List[str]: List of validation error messages (empty if valid)
    """
    errors = []

    # Column-level rules require column_qualified_name
    if spec.rule_type in COLUMN_LEVEL_RULES and not spec.column_qualified_name:
        errors.append(f"{spec.rule_type.value} requires column_qualified_name")

    # Custom SQL rules require specific fields
    if spec.rule_type == DQRuleType.CUSTOM_SQL:
        if not spec.custom_sql:
            errors.append("Custom SQL rules require custom_sql field")
        if not spec.rule_name:
            errors.append("Custom SQL rules require rule_name field")
        if not spec.dimension:
            errors.append("Custom SQL rules require dimension field")

    # Conditional rules should have conditions
    if spec.rule_type in CONDITIONAL_RULES and not spec.rule_conditions:
        logger.warning(f"{spec.rule_type.value} rule created without conditions")

    # Freshness rules require threshold_unit
    if spec.rule_type == DQRuleType.FRESHNESS and not spec.threshold_unit:
        errors.append(
            "Freshness rules require threshold_unit (DAYS, HOURS, or MINUTES)"
        )

    # All rules require threshold_value
    if spec.threshold_value is None:
        errors.append(f"{spec.rule_type.value} requires threshold_value")

    return errors


def _create_column_level_rule(spec: DQRuleSpecification, client) -> DataQualityRule:
    """
    Create a column-level data quality rule.

    Args:
        spec (DQRuleSpecification): Rule specification
        client: Atlan client instance

    Returns:
        DataQualityRule: Created rule asset
    """
    logger.debug(f"Creating column-level rule: {spec.rule_type.value}")

    # Prepare parameters
    params = {
        "client": client,
        "rule_type": spec.rule_type.value,
        "asset": Table.ref_by_qualified_name(qualified_name=spec.asset_qualified_name),
        "column": Column.ref_by_qualified_name(
            qualified_name=spec.column_qualified_name
        ),
        "threshold_value": spec.threshold_value,
        "alert_priority": DataQualityRuleAlertPriority[spec.alert_priority],
    }

    # Add optional parameters
    if spec.threshold_compare_operator:
        params["threshold_compare_operator"] = DataQualityRuleThresholdCompareOperator[
            spec.threshold_compare_operator
        ]

    if spec.threshold_unit:
        params["threshold_unit"] = DataQualityRuleThresholdUnit[spec.threshold_unit]

    if spec.row_scope_filtering_enabled:
        params["row_scope_filtering_enabled"] = spec.row_scope_filtering_enabled

    # Handle rule conditions for conditional rules
    if spec.rule_conditions and spec.rule_type in CONDITIONAL_RULES:
        rule_conditions = _build_rule_conditions(spec.rule_conditions)
        params["rule_conditions"] = rule_conditions

    # Create the rule
    dq_rule = DataQualityRule.column_level_rule_creator(**params)

    # Add description if provided
    if spec.description:
        dq_rule.description = spec.description

    return dq_rule


def _create_table_level_rule(spec: DQRuleSpecification, client) -> DataQualityRule:
    """
    Create a table-level data quality rule.

    Args:
        spec (DQRuleSpecification): Rule specification
        client: Atlan client instance

    Returns:
        DataQualityRule: Created rule asset
    """
    logger.debug(f"Creating table-level rule: {spec.rule_type.value}")

    # Prepare parameters
    params = {
        "client": client,
        "rule_type": spec.rule_type.value,
        "asset": Table.ref_by_qualified_name(qualified_name=spec.asset_qualified_name),
        "threshold_value": spec.threshold_value,
        "alert_priority": DataQualityRuleAlertPriority[spec.alert_priority],
    }

    # Add optional parameters
    if spec.threshold_compare_operator:
        params["threshold_compare_operator"] = DataQualityRuleThresholdCompareOperator[
            spec.threshold_compare_operator
        ]

    # Create the rule
    dq_rule = DataQualityRule.table_level_rule_creator(**params)

    # Add description if provided
    if spec.description:
        dq_rule.description = spec.description

    return dq_rule


def _create_custom_sql_rule(spec: DQRuleSpecification, client) -> DataQualityRule:
    """
    Create a custom SQL data quality rule.

    Args:
        spec (DQRuleSpecification): Rule specification
        client: Atlan client instance

    Returns:
        DataQualityRule: Created rule asset
    """
    logger.debug(f"Creating custom SQL rule: {spec.rule_name}")

    # Prepare parameters
    params = {
        "client": client,
        "rule_name": spec.rule_name,
        "asset": Table.ref_by_qualified_name(qualified_name=spec.asset_qualified_name),
        "custom_sql": spec.custom_sql,
        "threshold_value": spec.threshold_value,
        "alert_priority": DataQualityRuleAlertPriority[spec.alert_priority],
        "dimension": DataQualityDimension[spec.dimension],
    }

    # Add optional parameters
    if spec.threshold_compare_operator:
        params["threshold_compare_operator"] = DataQualityRuleThresholdCompareOperator[
            spec.threshold_compare_operator
        ]

    if spec.description:
        params["description"] = spec.description

    # Create the rule
    dq_rule = DataQualityRule.custom_sql_creator(**params)

    return dq_rule


def _build_rule_conditions(conditions: List) -> Any:
    """
    Build DQRuleConditionsBuilder from condition specifications.

    Args:
        conditions (List): List of DQRuleCondition objects or dicts

    Returns:
        Built rule conditions object
    """
    logger.debug(f"Building rule conditions for {len(conditions)} condition(s)")

    builder = DQRuleConditionsBuilder()

    for condition in conditions:
        # Handle dict format (now the only format)
        condition_type = DataQualityRuleTemplateConfigRuleConditions[condition["type"]]
        value = condition.get("value")
        min_value = condition.get("min_value")
        max_value = condition.get("max_value")

        # Build condition based on type
        condition_params = {"type": condition_type}

        if value is not None:
            condition_params["value"] = value

        if min_value is not None:
            condition_params["min_value"] = min_value

        if max_value is not None:
            condition_params["max_value"] = max_value

        builder.add_condition(**condition_params)

    return builder.build()
```

## modelcontextprotocol/tools/assets.py

```python
import logging
from typing import List, Union, Dict, Any
from client import get_atlan_client
from .models import (
    UpdatableAsset,
    UpdatableAttribute,
    CertificateStatus,
    TermOperation,
    TermOperations,
)
from pyatlan.model.assets import Readme, AtlasGlossaryTerm, AtlasGlossaryCategory
from pyatlan.model.fluent_search import CompoundQuery, FluentSearch

# Initialize logging
logger = logging.getLogger(__name__)


def update_assets(
    updatable_assets: Union[UpdatableAsset, List[UpdatableAsset]],
    attribute_name: UpdatableAttribute,
    attribute_values: List[Union[str, CertificateStatus, TermOperations]],
) -> Dict[str, Any]:
    """
    Update one or multiple assets with different values for attributes or term operations.

    Args:
        updatable_assets (Union[UpdatableAsset, List[UpdatableAsset]]): Asset(s) to update.
            Can be a single UpdatableAsset or a list of UpdatableAssets.
            For asset of type_name=AtlasGlossaryTerm or type_name=AtlasGlossaryCategory, each asset dictionary MUST include a "glossary_guid" key which is the GUID of the glossary that the term belongs to.
        attribute_name (UpdatableAttribute): Name of the attribute to update.
            Supports userDescription, certificateStatus, readme, and term.
        attribute_values (List[Union[str, CertificateStatus, TermOperations]]): List of values to set for the attribute.
            For certificateStatus, only VERIFIED, DRAFT, or DEPRECATED are allowed.
            For readme, the value must be a valid Markdown string.
            For term, the value must be a TermOperations object with operation and term_guids.

    Returns:
        Dict[str, Any]: Dictionary containing:
            - updated_count: Number of assets successfully updated
            - errors: List of any errors encountered
            - operation: The operation that was performed (for term operations)
    """
    try:
        # Convert single asset to list for consistent handling
        if not isinstance(updatable_assets, list):
            updatable_assets = [updatable_assets]

        logger.info(
            f"Updating {len(updatable_assets)} assets with attribute '{attribute_name}'"
        )

        # Validate attribute values
        if len(updatable_assets) != len(attribute_values):
            error_msg = "Number of asset GUIDs must match number of attribute values"
            logger.error(error_msg)
            return {"updated_count": 0, "errors": [error_msg]}

        # Initialize result tracking
        result = {"updated_count": 0, "errors": []}

        # Validate certificate status values if applicable
        if attribute_name == UpdatableAttribute.CERTIFICATE_STATUS:
            for value in attribute_values:
                if value not in CertificateStatus.__members__.values():
                    error_msg = f"Invalid certificate status: {value}"
                    logger.error(error_msg)
                    result["errors"].append(error_msg)

        # Get Atlan client
        client = get_atlan_client()

        # Create assets with updated values
        assets = []
        # readme_update_parent_assets: Assets that were updated with readme.
        readme_update_parent_assets = []
        for index, updatable_asset in enumerate(updatable_assets):
            type_name = updatable_asset.type_name
            qualified_name = updatable_asset.qualified_name
            asset_cls = getattr(
                __import__("pyatlan.model.assets", fromlist=[type_name]), type_name
            )

            # Special handling for Glossary Term updates
            if (
                updatable_asset.type_name == AtlasGlossaryTerm.__name__
                or updatable_asset.type_name == AtlasGlossaryCategory.__name__
            ):
                asset = asset_cls.updater(
                    qualified_name=updatable_asset.qualified_name,
                    name=updatable_asset.name,
                    glossary_guid=updatable_asset.glossary_guid,
                )
            else:
                asset = asset_cls.updater(
                    qualified_name=updatable_asset.qualified_name,
                    name=updatable_asset.name,
                )

            # Special handling for README updates
            if attribute_name == UpdatableAttribute.README:
                # Get the current readme content for the asset
                # The below query is used to get the asset based on the qualified name and include the readme content.
                asset_readme_response = (
                    FluentSearch()
                    .select()
                    .where(CompoundQuery.asset_type(asset_cls))
                    .where(asset_cls.QUALIFIED_NAME.eq(qualified_name))
                    .include_on_results(asset_cls.README)
                    .include_on_relations(Readme.DESCRIPTION)
                    .execute(client=client)
                )

                if first := asset_readme_response.current_page():
                    updated_content = attribute_values[index]
                    # We replace the existing readme content with the new content.
                    # If the existing readme content is not present, we create a new readme asset.
                    updated_readme = Readme.creator(
                        asset=first[0], content=updated_content
                    )
                    # Save the readme asset
                    assets.append(updated_readme)
                    # Add the parent/actual asset to the list of assets that were updated with readme.
                    readme_update_parent_assets.append(asset)
            elif attribute_name == UpdatableAttribute.TERM:
                # Special handling for term operations
                term_value = attribute_values[index]
                if not isinstance(term_value, TermOperations):
                    error_msg = f"Term value must be a TermOperations object for asset {updatable_asset.qualified_name}"
                    logger.error(error_msg)
                    result["errors"].append(error_msg)
                    continue

                term_operation = TermOperation(term_value.operation.lower())
                term_guids = term_value.term_guids

                # Create term references
                term_refs = [
                    AtlasGlossaryTerm.ref_by_guid(guid=guid) for guid in term_guids
                ]

                try:
                    # Perform the appropriate term operation
                    if term_operation == TermOperation.APPEND:
                        client.asset.append_terms(
                            asset_type=asset_cls,
                            qualified_name=updatable_asset.qualified_name,
                            terms=term_refs,
                        )
                    elif term_operation == TermOperation.REPLACE:
                        client.asset.replace_terms(
                            asset_type=asset_cls,
                            qualified_name=updatable_asset.qualified_name,
                            terms=term_refs,
                        )
                    elif term_operation == TermOperation.REMOVE:
                        client.asset.remove_terms(
                            asset_type=asset_cls,
                            qualified_name=updatable_asset.qualified_name,
                            terms=term_refs,
                        )

                    result["updated_count"] += 1
                    logger.info(
                        f"Successfully {term_operation.value}d terms on asset: {updatable_asset.qualified_name}"
                    )

                except Exception as e:
                    error_msg = f"Error updating terms on asset {updatable_asset.qualified_name}: {str(e)}"
                    logger.error(error_msg)
                    result["errors"].append(error_msg)
            else:
                # Regular attribute update flow
                setattr(asset, attribute_name.value, attribute_values[index])
                assets.append(asset)

        if len(readme_update_parent_assets) > 0:
            result["readme_updated"] = len(readme_update_parent_assets)
            # Collect qualified names or other identifiers for assets that were updated with readme
            result["updated_readme_assets"] = [
                asset.qualified_name
                for asset in readme_update_parent_assets
                if hasattr(asset, "qualified_name")
            ]
            logger.info(
                f"Successfully updated {result['readme_updated']} readme assets: {result['updated_readme_assets']}"
            )

        # Proces response
        if len(assets) > 0:
            response = client.asset.save(assets)
            result["updated_count"] = len(response.guid_assignments)
        logger.info(f"Successfully updated {result['updated_count']} assets")

        return result

    except Exception as e:
        error_msg = f"Error updating assets: {str(e)}"
        logger.error(error_msg)
        return {"updated_count": 0, "errors": [error_msg]}
```

## modelcontextprotocol/tools/search.py

```python
import logging
from typing import Type, List, Optional, Union, Dict, Any

from client import get_atlan_client
from pyatlan.model.assets import Asset, AtlasGlossaryTerm
from pyatlan.model.fluent_search import CompoundQuery, FluentSearch
from pyatlan.model.fields.atlan_fields import AtlanField
from utils.search import SearchUtils
from utils.constants import DEFAULT_SEARCH_ATTRIBUTES, VALID_RELATIONSHIPS

# Configure logging
logger = logging.getLogger(__name__)


def search_assets(
    conditions: Optional[Union[Dict[str, Any], str]] = None,
    negative_conditions: Optional[Dict[str, Any]] = None,
    some_conditions: Optional[Dict[str, Any]] = None,
    min_somes: int = 1,
    include_attributes: Optional[List[Union[str, AtlanField]]] = None,
    asset_type: Optional[Union[Type[Asset], str]] = None,
    include_archived: bool = False,
    limit: int = 10,
    offset: int = 0,
    sort_by: Optional[str] = None,
    sort_order: str = "ASC",
    connection_qualified_name: Optional[str] = None,
    tags: Optional[List[str]] = None,
    directly_tagged: bool = True,
    domain_guids: Optional[List[str]] = None,
    date_range: Optional[Dict[str, Dict[str, Any]]] = None,
    guids: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """
    Advanced asset search using FluentSearch with flexible conditions.

    By default, only essential attributes used in result processing are included.
    Additional attributes can be specified via include_attributes parameter.

    Args:
        conditions (Dict[str, Any], optional): Dictionary of attribute conditions to match.
            Format: {"attribute_name": value} or {"attribute_name": {"operator": operator, "value": value}}
        negative_conditions (Dict[str, Any], optional): Dictionary of attribute conditions to exclude.
            Format: {"attribute_name": value} or {"attribute_name": {"operator": operator, "value": value}}
        some_conditions (Dict[str, Any], optional): Conditions for where_some() queries that require min_somes of them to match.
            Format: {"attribute_name": value} or {"attribute_name": {"operator": operator, "value": value}}
        min_somes (int): Minimum number of some_conditions that must match. Defaults to 1.
        include_attributes (List[Union[str, AtlanField]], optional): List of additional attributes to include in results.
            Can be string attribute names or AtlanField objects. These will be added to the default set.
        asset_type (Union[Type[Asset], str], optional): Type of asset to search for.
            Either a class (e.g., Table, Column) or a string type name (e.g., "Table", "Column")
        include_archived (bool): Whether to include archived assets. Defaults to False.
        limit (int, optional): Maximum number of results to return. Defaults to 10.
        offset (int, optional): Offset for pagination. Defaults to 0.
        sort_by (str, optional): Attribute to sort by. Defaults to None.
        sort_order (str, optional): Sort order, "ASC" or "DESC". Defaults to "ASC".
        connection_qualified_name (str, optional): Connection qualified name to filter by.
        tags (List[str], optional): List of tags to filter by.
        directly_tagged (bool): Whether to filter for directly tagged assets only. Defaults to True.
        domain_guids (List[str], optional): List of domain GUIDs to filter by.
        date_range (Dict[str, Dict[str, Any]], optional): Date range filters.
            Format: {"attribute_name": {"gte": start_timestamp, "lte": end_timestamp}}
        guids (List[str], optional): List of GUIDs to filter by.


    Returns:
        Dict[str, Any]: Dictionary containing:
            - results: List of assets matching the search criteria
            - aggregations: Search aggregations if available
            - error: None if no error occurred, otherwise the error message
    """
    logger.info(
        f"Starting asset search with parameters: asset_type={asset_type}, "
        f"limit={limit}, include_archived={include_archived}"
    )
    logger.debug(
        f"Full search parameters: conditions={conditions}, "
        f"negative_conditions={negative_conditions}, some_conditions={some_conditions}, "
        f"include_attributes={include_attributes}, "
        f"connection_qualified_name={connection_qualified_name}, "
        f"tags={tags}, domain_guids={domain_guids}"
    )

    try:
        # Initialize FluentSearch
        logger.debug("Initializing FluentSearch object")
        search = FluentSearch()

        # Apply asset type filter if provided
        if asset_type:
            if isinstance(asset_type, str):
                # Handle string type name
                logger.debug(f"Filtering by asset type name: {asset_type}")
                search = search.where(Asset.TYPE_NAME.eq(asset_type))
            else:
                # Handle class type
                logger.debug(f"Filtering by asset class: {asset_type.__name__}")
                search = search.where(CompoundQuery.asset_type(asset_type))

        # Filter for active assets unless archived are explicitly included
        if not include_archived:
            logger.debug("Filtering for active assets only")
            search = search.where(CompoundQuery.active_assets())

        # Apply connection qualified name filter if provided
        if connection_qualified_name:
            logger.debug(
                f"Filtering by connection qualified name: {connection_qualified_name}"
            )
            search = search.where(
                Asset.QUALIFIED_NAME.startswith(connection_qualified_name)
            )

        # Apply tags filter if provided
        if tags and len(tags) > 0:
            logger.debug(
                f"Filtering by tags: {tags}, directly_tagged={directly_tagged}"
            )
            search = search.where(
                CompoundQuery.tagged(with_one_of=tags, directly=directly_tagged)
            )

        # Apply domain GUIDs filter if provided
        if domain_guids and len(domain_guids) > 0:
            logger.debug(f"Filtering by domain GUIDs: {domain_guids}")
            for guid in domain_guids:
                search = search.where(Asset.DOMAIN_GUIDS.eq(guid))

        # Apply positive conditions
        if conditions:
            if not isinstance(conditions, dict):
                error_msg = f"Conditions parameter must be a dictionary, got {type(conditions).__name__}"
                logger.error(error_msg)
                return []

            logger.debug(f"Applying positive conditions: {conditions}")
            for attr_name, condition in conditions.items():
                attr = SearchUtils._get_asset_attribute(attr_name)
                if attr is None:
                    logger.warning(
                        f"Unknown attribute: {attr_name}, skipping condition"
                    )
                    continue

                logger.debug(f"Processing condition for attribute: {attr_name}")

                search = SearchUtils._process_condition(
                    search, attr, condition, attr_name, "where"
                )

        # Apply negative conditions
        if negative_conditions:
            logger.debug(f"Applying negative conditions: {negative_conditions}")
            for attr_name, condition in negative_conditions.items():
                attr = SearchUtils._get_asset_attribute(attr_name)
                if attr is None:
                    logger.warning(
                        f"Unknown attribute for negative condition: {attr_name}, skipping"
                    )
                    continue

                logger.debug(
                    f"Processing negative condition for attribute: {attr_name}"
                )

                search = SearchUtils._process_condition(
                    search, attr, condition, attr_name, "where_not"
                )

        # Apply where_some conditions with min_somes
        if some_conditions:
            logger.debug(
                f"Applying 'some' conditions: {some_conditions} with min_somes={min_somes}"
            )
            for attr_name, condition in some_conditions.items():
                attr = SearchUtils._get_asset_attribute(attr_name)
                if attr is None:
                    logger.warning(
                        f"Unknown attribute for 'some' condition: {attr_name}, skipping"
                    )
                    continue

                logger.debug(f"Processing 'some' condition for attribute: {attr_name}")

                search = SearchUtils._process_condition(
                    search, attr, condition, attr_name, "where_some"
                )
            search = search.min_somes(min_somes)

        # Apply date range filters
        if date_range:
            logger.debug(f"Applying date range filters: {date_range}")
            date_range_count = 0
            for attr_name, range_cond in date_range.items():
                attr = SearchUtils._get_asset_attribute(attr_name)
                if attr is None:
                    logger.warning(
                        f"Unknown attribute for date range: {attr_name}, skipping"
                    )
                    continue

                logger.debug(f"Processing date range for attribute: {attr_name}")

                if "gte" in range_cond:
                    logger.debug(f"Adding {attr_name} >= {range_cond['gte']}")
                    search = search.where(attr.gte(range_cond["gte"]))
                    date_range_count += 1
                if "lte" in range_cond:
                    logger.debug(f"Adding {attr_name} <= {range_cond['lte']}")
                    search = search.where(attr.lte(range_cond["lte"]))
                    date_range_count += 1
                if "gt" in range_cond:
                    logger.debug(f"Adding {attr_name} > {range_cond['gt']}")
                    search = search.where(attr.gt(range_cond["gt"]))
                    date_range_count += 1
                if "lt" in range_cond:
                    logger.debug(f"Adding {attr_name} < {range_cond['lt']}")
                    search = search.where(attr.lt(range_cond["lt"]))
                    date_range_count += 1

            logger.debug(f"Applied {date_range_count} date range conditions")

        if guids and len(guids) > 0:
            logger.debug(f"Applying GUID filter: {guids}")
            search = search.where(Asset.GUID.within(guids))

        # Prepare attributes to include: default attributes + additional user-specified attributes
        all_attributes = DEFAULT_SEARCH_ATTRIBUTES.copy()

        if include_attributes:
            logger.debug(f"Adding user-specified attributes: {include_attributes}")
            for attr in include_attributes:
                if isinstance(attr, str):
                    if attr not in all_attributes:
                        all_attributes.append(attr)
                else:
                    # For AtlanField objects, we'll add them directly to the search
                    # They can't be easily compared for duplicates
                    pass

        logger.debug(f"Total attributes to include: {all_attributes}")

        # Include all attributes in results
        for attr_name in all_attributes:
            attr_obj = SearchUtils._get_asset_attribute(attr_name)
            if attr_obj is None:
                logger.warning(
                    f"Unknown attribute for inclusion: {attr_name}, skipping"
                )
                continue
            logger.debug(f"Including attribute: {attr_name}")
            search = search.include_on_results(attr_obj)

        # Include additional AtlanField objects specified by user
        if include_attributes:
            for attr in include_attributes:
                if not isinstance(attr, str):
                    # Assume it's already an AtlanField object
                    logger.debug(f"Including attribute object: {attr}")
                    search = search.include_on_results(attr)
                elif attr in VALID_RELATIONSHIPS:
                    search = search.include_on_results(attr)
        try:
            search = search.include_on_results(Asset.ASSIGNED_TERMS)
            search = search.include_on_relations(AtlasGlossaryTerm.NAME)
        except Exception as e:
            logger.warning(f"Error including assigned terms: {e}")

        # Set pagination
        logger.debug(f"Setting pagination: limit={limit}, offset={offset}")
        search = search.page_size(limit)
        if offset > 0:
            search = search.from_offset(offset)

        # Set sorting
        if sort_by:
            sort_attr = SearchUtils._get_asset_attribute(sort_by)
            if sort_attr is not None:
                if sort_order.upper() == "DESC":
                    logger.debug(f"Setting sort order: {sort_by} DESC")
                    search = search.sort_by_desc(sort_attr)
                else:
                    logger.debug(f"Setting sort order: {sort_by} ASC")
                    search = search.sort_by_asc(sort_attr)
            else:
                logger.warning(
                    f"Unknown attribute for sorting: {sort_by}, skipping sort"
                )

        # Execute search
        logger.debug("Converting FluentSearch to request object")
        request = search.to_request()

        logger.info("Executing search request")
        client = get_atlan_client()
        search_response = client.asset.search(request)
        processed_results = SearchUtils.process_results(search_response)
        logger.info(
            f"Search completed, returned {len(processed_results['results'])} results"
        )
        return processed_results

    except Exception as e:
        logger.error(f"Error searching assets: {str(e)}")
        return [{"results": [], "aggregations": {}, "error": str(e)}]
```

## modelcontextprotocol/tools/lineage.py

```python
import logging
from typing import Dict, Any, List, Optional, Union

from client import get_atlan_client
from pyatlan.model.enums import LineageDirection
from pyatlan.model.lineage import FluentLineage
from pyatlan.model.fields.atlan_fields import AtlanField
from utils.search import SearchUtils
from utils.constants import DEFAULT_SEARCH_ATTRIBUTES

# Configure logging
logger = logging.getLogger(__name__)


def traverse_lineage(
    guid: str,
    direction: LineageDirection,
    depth: int = 1000000,
    size: int = 10,
    immediate_neighbors: bool = False,
    include_attributes: Optional[List[Union[str, AtlanField]]] = None,
) -> Dict[str, Any]:
    """
    Traverse asset lineage in specified direction.

    By default, essential attributes used in search operations are included.
    Additional attributes can be specified via include_attributes parameter.

    Args:
        guid (str): GUID of the starting asset
        direction (LineageDirection): Direction to traverse (UPSTREAM or DOWNSTREAM)
        depth (int, optional): Maximum depth to traverse. Defaults to 1000000.
        size (int, optional): Maximum number of results to return. Defaults to 10.
        immediate_neighbors (bool, optional): Only return immediate neighbors. Defaults to False.
        include_attributes (Optional[List[Union[str, AtlanField]]], optional): List of additional
            attributes to include in results. Can be string attribute names or AtlanField objects.
            These will be added to the default set. Defaults to None.

    Returns:
        Dict[str, Any]: Dictionary containing:
            - assets: List of assets in the lineage with processed attributes
            - error: None if no error occurred, otherwise the error message

    Raises:
        Exception: If there's an error executing the lineage request
    """
    logger.info(
        f"Starting lineage traversal from {guid} in direction {direction}, "
        f"depth={depth}, size={size}, immediate_neighbors={immediate_neighbors}"
    )
    logger.debug(f"Include attributes parameter: {include_attributes}")

    try:
        # Initialize base request
        logger.debug("Initializing FluentLineage object")
        lineage_builder = (
            FluentLineage(starting_guid=guid)
            .direction(direction)
            .depth(depth)
            .size(size)
            .immediate_neighbors(immediate_neighbors)
        )

        # Prepare attributes to include: default attributes + additional user-specified attributes
        all_attributes = DEFAULT_SEARCH_ATTRIBUTES.copy()

        if include_attributes:
            logger.debug(f"Adding user-specified attributes: {include_attributes}")
            for attr in include_attributes:
                if isinstance(attr, str) and attr not in all_attributes:
                    all_attributes.append(attr)

        logger.debug(f"Total attributes to include: {all_attributes}")

        # Include all string attributes in results
        for attr_name in all_attributes:
            attr_obj = SearchUtils._get_asset_attribute(attr_name)
            if attr_obj is None:
                logger.warning(
                    f"Unknown attribute for inclusion: {attr_name}, skipping"
                )
                continue
            logger.debug(f"Including attribute: {attr_name}")
            lineage_builder = lineage_builder.include_on_results(attr_obj)

        # Execute request
        logger.debug("Converting FluentLineage to request object")
        request = lineage_builder.request

        logger.info("Executing lineage request")
        client = get_atlan_client()
        response = client.asset.get_lineage_list(request)

        # Process results using same pattern as search
        logger.info("Processing lineage results")
        if response is None:
            logger.info("No lineage results found")
            return {"assets": [], "error": None}

        # Convert results to list and process using Pydantic serialization
        results_list = [
            result.dict(by_alias=True, exclude_unset=True)
            for result in response
            if result is not None
        ]

        logger.info(
            f"Lineage traversal completed, returned {len(results_list)} results"
        )
        return {"assets": results_list, "error": None}

    except Exception as e:
        logger.error(f"Error traversing lineage: {str(e)}")
        return {"assets": [], "error": str(e)}
```

## modelcontextprotocol/tools/glossary.py

```python
from __future__ import annotations
import logging
from typing import Dict, Any, List, Union

from pyatlan.model.assets import (
    AtlasGlossary,
    AtlasGlossaryCategory,
    AtlasGlossaryTerm,
    Asset,
)
from utils.parameters import parse_list_parameter
from client import get_atlan_client
from .models import (
    CertificateStatus,
    Glossary,
    GlossaryCategory,
    GlossaryTerm,
)

logger = logging.getLogger(__name__)


def save_assets(assets: List[Asset]) -> List[Dict[str, Any]]:
    """
    Common bulk save and response processing for any asset type.

    Args:
        assets (List[Asset]): List of Asset objects to save.

    Returns:
        List[Dict[str, Any]]: List of dictionaries with details for each created asset.

    Raises:
        Exception: If there's an error saving the assets.
    """
    logger.info("Starting bulk save operation")
    client = get_atlan_client()
    try:
        response = client.asset.save(assets)
    except Exception as e:
        logger.error(f"Error saving assets: {e}")
        raise e
    results: List[Dict[str, Any]] = []
    created_assets = response.mutated_entities.CREATE

    logger.info(f"Save operation completed, processing {len(created_assets)} results")

    results = [
        {
            "guid": created_asset.guid,
            "name": created_asset.name,
            "qualified_name": created_asset.qualified_name,
        }
        for created_asset in created_assets
    ]

    logger.info(f"Bulk save completed successfully for {len(results)} assets")
    return results


def create_glossary_assets(
    glossaries: Union[Dict[str, Any], List[Dict[str, Any]]],
) -> List[Dict[str, Any]]:
    """
    Create one or multiple AtlasGlossary assets in Atlan.

    Args:
        glossaries (Union[Dict[str, Any], List[Dict[str, Any]]]): Either a single glossary
            specification (dict) or a list of glossary specifications. Each specification
            can be a dictionary containing:
            - name (str): Name of the glossary (required)
            - user_description (str, optional): Detailed description of the glossary
              proposed by the user
            - certificate_status (str, optional): Certification status
              ("VERIFIED", "DRAFT", or "DEPRECATED")

    Returns:
        List[Dict[str, Any]]: List of dictionaries, each with details for a created glossary:
            - guid: The GUID of the created glossary
            - name: The name of the glossary
            - qualified_name: The qualified name of the created glossary

    Raises:
        Exception: If there's an error creating the glossary assets.
    """
    data = glossaries if isinstance(glossaries, list) else [glossaries]
    logger.info(f"Creating {len(data)} glossary asset(s)")
    logger.debug(f"Glossary specifications: {data}")

    specs = [Glossary(**item) for item in data]

    assets: List[AtlasGlossary] = []
    for spec in specs:
        logger.debug(f"Creating AtlasGlossary for: {spec.name}")
        glossary = AtlasGlossary.creator(name=spec.name)
        glossary.user_description = spec.user_description
        if spec.certificate_status is not None:
            cs = (
                CertificateStatus(spec.certificate_status)
                if isinstance(spec.certificate_status, str)
                else spec.certificate_status
            )
            glossary.certificate_status = cs.value
            logger.debug(f"Set certificate status for {spec.name}: {cs.value}")
        assets.append(glossary)

    return save_assets(assets)


def create_glossary_category_assets(
    categories: Union[Dict[str, Any], List[Dict[str, Any]]],
) -> List[Dict[str, Any]]:
    """
    Create one or multiple AtlasGlossaryCategory assets in Atlan.

    Args:
        categories (Union[Dict[str, Any], List[Dict[str, Any]]]): Either a single category
            specification (dict) or a list of category specifications. Each specification
            can be a dictionary containing:
            - name (str): Name of the category (required)
            - glossary_guid (str): GUID of the glossary this category belongs to (required)
            - user_description (str, optional): Detailed description of the category
              proposed by the user
            - certificate_status (str, optional): Certification status
              ("VERIFIED", "DRAFT", or "DEPRECATED")
            - parent_category_guid (str, optional): GUID of the parent category if this
              is a subcategory

    Returns:
        List[Dict[str, Any]]: List of dictionaries, each with details for a created category:
            - guid: The GUID of the created category
            - name: The name of the category
            - qualified_name: The qualified name of the created category

    Raises:
        Exception: If there's an error creating the glossary category assets.
    """
    data = categories if isinstance(categories, list) else [categories]
    logger.info(f"Creating {len(data)} glossary category asset(s)")
    logger.debug(f"Category specifications: {data}")

    specs = [GlossaryCategory(**item) for item in data]

    assets: List[AtlasGlossaryCategory] = []
    for spec in specs:
        logger.debug(f"Creating AtlasGlossaryCategory for: {spec.name}")
        anchor = AtlasGlossary.ref_by_guid(spec.glossary_guid)
        category = AtlasGlossaryCategory.creator(
            name=spec.name,
            anchor=anchor,
            parent_category=(
                AtlasGlossaryCategory.ref_by_guid(spec.parent_category_guid)
                if spec.parent_category_guid
                else None
            ),
        )
        category.user_description = spec.user_description
        if spec.certificate_status is not None:
            cs = (
                CertificateStatus(spec.certificate_status)
                if isinstance(spec.certificate_status, str)
                else spec.certificate_status
            )
            category.certificate_status = cs.value
            logger.debug(f"Set certificate status for {spec.name}: {cs.value}")

        assets.append(category)

    return save_assets(assets)


def create_glossary_term_assets(
    terms: Union[Dict[str, Any], List[Dict[str, Any]]],
) -> List[Dict[str, Any]]:
    """
    Create one or multiple AtlasGlossaryTerm assets in Atlan.

    Args:
        terms (Union[Dict[str, Any], List[Dict[str, Any]]]): Either a single term
            specification (dict) or a list of term specifications. Each specification
            can be a dictionary containing:
            - name (str): Name of the term (required)
            - glossary_guid (str): GUID of the glossary this term belongs to (required)
            - user_description (str, optional): Detailed description of the term
              proposed by the user
            - certificate_status (str, optional): Certification status
              ("VERIFIED", "DRAFT", or "DEPRECATED")
            - category_guids (List[str], optional): List of category GUIDs this term
              belongs to

    Returns:
        List[Dict[str, Any]]: List of dictionaries, each with details for a created term:
            - guid: The GUID of the created term
            - name: The name of the term
            - qualified_name: The qualified name of the created term

    Raises:
        ValueError: If any provided category_guids are not found.
        Exception: If there's an error creating the glossary term assets.
    """
    data = terms if isinstance(terms, list) else [terms]
    logger.info(f"Creating {len(data)} glossary term asset(s)")
    logger.debug(f"Term specifications: {data}")

    specs = [GlossaryTerm(**item) for item in data]
    per_term_guids = [set(parse_list_parameter(s.category_guids) or []) for s in specs]

    assets: List[AtlasGlossaryTerm] = []
    for spec, guids in zip(specs, per_term_guids):
        term = AtlasGlossaryTerm.creator(
            name=spec.name,
            anchor=AtlasGlossary.ref_by_guid(spec.glossary_guid),
            categories=[AtlasGlossaryCategory.ref_by_guid(g) for g in guids] or None,
        )
        term.user_description = spec.user_description
        if spec.certificate_status is not None:
            cs = (
                CertificateStatus(spec.certificate_status)
                if isinstance(spec.certificate_status, str)
                else spec.certificate_status
            )
            term.certificate_status = cs.value
        assets.append(term)

    return save_assets(assets)
```

## modelcontextprotocol/version.py

```python
"""Version information."""

__version__ = "0.2.11"
```

## modelcontextprotocol/Dockerfile

```text
# Use a Python image with uv pre-installed
FROM ghcr.io/astral-sh/uv:python3.12-bookworm-slim AS builder

# Set environment variables for build
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

# Install the project into `/app`
WORKDIR /app

ADD . /app

# Create a virtual environment and install dependencies
RUN python -m venv /app/.venv
ENV PATH="/app/.venv/bin:$PATH"
RUN uv sync --no-cache-dir --no-dev --python /app/.venv/bin/python

FROM python:3.12-slim-bookworm AS runtime

RUN groupadd -r appuser && useradd -r -g appuser -m -d /home/appuser appuser

WORKDIR /appuser

COPY --from=builder --chown=appuser:appuser /app /appuser

# Set the PATH to use the virtual environment
ENV PATH="/appuser/.venv/bin:$PATH"

ENV MCP_TRANSPORT="stdio"
ENV MCP_HOST="0.0.0.0"
ENV MCP_PORT="8000"
ENV MCP_PATH="/"

USER appuser

ENTRYPOINT exec python server.py --transport "$MCP_TRANSPORT" --host "$MCP_HOST" --port "$MCP_PORT" --path "$MCP_PATH"
```

## modelcontextprotocol/client.py

```python
"""Client factory for Atlan."""

import logging
from typing import Optional

from pyatlan.client.atlan import AtlanClient
from settings import get_settings

logger = logging.getLogger(__name__)

_client_instance: Optional[AtlanClient] = None


def get_atlan_client() -> AtlanClient:
    """
    Get the singleton AtlanClient instance for connection reuse.

    Returns:
        AtlanClient: The singleton AtlanClient instance.

    Raises:
        Exception: If client creation fails.
    """
    global _client_instance

    if _client_instance is None:
        settings = get_settings()
        try:
            _client_instance = AtlanClient(
                base_url=settings.ATLAN_BASE_URL, api_key=settings.ATLAN_API_KEY
            )
            _client_instance.update_headers(settings.headers)
            logger.info("AtlanClient initialized successfully")
        except Exception:
            logger.error("Failed to create Atlan client", exc_info=True)
            raise

    return _client_instance
```

## modelcontextprotocol/pyproject.toml

```text
[project]
name = "atlan-mcp-server"
dynamic = ["version"]
description = "Atlan Model Context Protocol server for interacting with Atlan services"
readme = "README.md"
requires-python = ">=3.11"
license = { text = "MIT" }
authors = [
    {name = "AtlanHQ", email = "engineering@atlan.com"}
]
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]

dependencies = [
    "fastmcp==2.13.0.2",
    "pyatlan>=6.0.1",
    "uvicorn>=0.35.0"
]

[project.scripts]
atlan-mcp-server = "server:main"

[project.urls]
"Homepage" = "https://github.com/atlanhq/agent-toolkit"
"Documentation" = "https://ask.atlan.com/hc/en-us/articles/12525731740175-How-to-implement-the-Atlan-MCP-server"
"Bug Tracker" = "https://github.com/atlanhq/agent-toolkit/issues"
"Source" = "https://github.com/atlanhq/agent-toolkit.git"
"Changelog" = "https://github.com/atlanhq/agent-toolkit/blob/main/CHANGELOG.md"

[tool.hatch.version]
path = "version.py"

[tool.hatch.build.targets.wheel]
packages = ["."]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

## modelcontextprotocol/utils/constants.py

```python
VALID_RELATIONSHIPS = ["anchor"]

DEFAULT_SEARCH_ATTRIBUTES = [
    "name",
    "display_name",
    "description",
    "qualified_name",
    "user_description",
    "certificate_status",
    "owner_users",
    "connector_name",
    "has_lineage",
    "source_created_at",
    "source_updated_at",
    "readme",
    "owner_groups",
    "asset_tags",
]
```

## modelcontextprotocol/utils/__init__.py

```python
"""
Utilities for the Atlan MCP server.

This package provides common utilities used across the server components.
"""

from .constants import DEFAULT_SEARCH_ATTRIBUTES
from .search import SearchUtils
from .parameters import (
    parse_json_parameter,
    parse_list_parameter,
)

__all__ = [
    "DEFAULT_SEARCH_ATTRIBUTES",
    "SearchUtils",
    "parse_json_parameter",
    "parse_list_parameter",
]
```

## modelcontextprotocol/utils/parameters.py

```python
"""
Parameter parsing and validation utilities for MCP tools.

This module provides reusable functions for parsing and validating
parameters that are commonly used across different MCP tools.
"""

import json
import logging
from typing import Any, List, Optional, Union

logger = logging.getLogger(__name__)


def parse_json_parameter(param: Any) -> Union[dict, list, None]:
    """
    Parse a parameter that might be a JSON string.

    Args:
        param: The parameter value to parse (could be string, dict, list, etc.)

    Returns:
        The parsed parameter value

    Raises:
        json.JSONDecodeError: If the JSON string is invalid
    """
    if param is None:
        return None

    if isinstance(param, str):
        try:
            return json.loads(param)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON parameter: {param}")
            raise e

    return param


def parse_list_parameter(param: Any) -> Optional[List[Any]]:
    """
    Parse a parameter that might be a JSON string representing a list.

    Args:
        param: The parameter value to parse

    Returns:
        The parsed list, None if param is None, or original value converted to list if needed

    Raises:
        json.JSONDecodeError: If the JSON string is invalid
    """
    if param is None:
        return None

    if isinstance(param, str):
        try:
            parsed = json.loads(param)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON parameter: {param}")
            raise e

        if isinstance(parsed, list):
            return parsed
        return [parsed]

    if isinstance(param, list):
        return param

    return [param]
```

## modelcontextprotocol/utils/search.py

```python
from typing import Dict, Any
import logging
from pyatlan.model.assets import Asset

logger = logging.getLogger(__name__)


class SearchUtils:
    @staticmethod
    def process_results(results: Any) -> Dict[str, Any]:
        """
        Process the results from the search index using Pydantic serialization.

        This method uses Pydantic's .dict(by_alias=True, exclude_unset=True) to:
        - Convert field names to their API-friendly camelCase format (by_alias=True)
        - Exclude any fields that weren't explicitly set (exclude_unset=True)

        Args:
            results: The search results from Atlan

        Returns:
            Dict[str, Any]: Dictionary containing:
                - results: List of processed results
                - aggregations: Search aggregations if available
                - error: None if no error occurred, otherwise the error message
        """
        current_page_results = (
            results.current_page()
            if hasattr(results, "current_page") and callable(results.current_page)
            else []
        )
        aggregations = results.aggregations

        logger.info(f"Processing {len(current_page_results)} search results")
        results_list = [
            result.dict(by_alias=True, exclude_unset=True)
            for result in current_page_results
            if result is not None
        ]

        return {"results": results_list, "aggregations": aggregations, "error": None}

    @staticmethod
    def _get_asset_attribute(attr_name: str):
        """
        Get Asset attribute by name.
        """
        return getattr(Asset, attr_name.upper(), None)

    @staticmethod
    def _apply_operator_condition(
        attr, operator: str, value: Any, case_insensitive: bool = False
    ):
        """
        Apply an operator condition to an attribute.

        Args:
            attr: The Asset attribute object
            operator (str): The operator to apply
            value: The value for the condition
            case_insensitive (bool): Whether to apply case insensitive matching

        Returns:
            The condition object to be used with where/where_not/where_some

        Raises:
            ValueError: If the operator is unknown or value format is invalid
        """
        logger.debug(
            f"Applying operator '{operator}' with value '{value}' (case_insensitive={case_insensitive})"
        )

        if operator == "startswith":
            return attr.startswith(value, case_insensitive=case_insensitive)
        elif operator == "match":
            return attr.match(value)
        elif operator == "eq":
            return attr.eq(value, case_insensitive=case_insensitive)
        elif operator == "neq":
            return attr.neq(value, case_insensitive=case_insensitive)
        elif operator == "gte":
            return attr.gte(value)
        elif operator == "lte":
            return attr.lte(value)
        elif operator == "gt":
            return attr.gt(value)
        elif operator == "lt":
            return attr.lt(value)
        elif operator == "has_any_value":
            return attr.has_any_value()
        elif operator == "contains":
            return attr.contains(value, case_insensitive=case_insensitive)
        elif operator == "between":
            # Expecting value to be a list/tuple with [start, end]
            if isinstance(value, (list, tuple)) and len(value) == 2:
                return attr.between(value[0], value[1])
            else:
                raise ValueError(
                    f"Invalid value format for 'between' operator: {value}, expected [start, end]"
                )
        else:
            # Try to get the operator method from the attribute
            op_method = getattr(attr, operator, None)
            if op_method is None:
                raise ValueError(f"Unknown operator: {operator}")

            # Try to pass case_insensitive if the method supports it
            try:
                return op_method(value, case_insensitive=case_insensitive)
            except TypeError:
                # Fallback if case_insensitive is not supported
                return op_method(value)

    @staticmethod
    def _process_condition(
        search, attr, condition, attr_name: str, search_method_name: str
    ):
        """
        Process a single condition and apply it to the search using the specified method.

        Args:
            search: The FluentSearch object
            attr: The Asset attribute object
            condition: The condition value (dict, list, or simple value)
            attr_name (str): The attribute name for logging
            search_method_name (str): The search method to use ('where', 'where_not', 'where_some')

        Returns:
            FluentSearch: The updated search object
        """
        search_method = getattr(search, search_method_name)

        if isinstance(condition, dict):
            operator = condition.get("operator", "eq")
            value = condition.get("value")
            case_insensitive = condition.get("case_insensitive", False)

            try:
                condition_obj = SearchUtils._apply_operator_condition(
                    attr, operator, value, case_insensitive
                )
                search = search_method(condition_obj)
                return search
            except ValueError as e:
                logger.warning(f"Skipping condition for {attr_name}: {e}")
                return search
        elif isinstance(condition, list):
            if search_method_name == "where_some":
                # Handle multiple values for where_some
                logger.debug(
                    f"Adding multiple '{search_method_name}' values for {attr_name}: {condition}"
                )
                for value in condition:
                    search = search_method(attr.eq(value))
                return search
            else:
                # Handle list of values with OR logic using .within()
                logger.debug(f"Applying multiple values for {attr_name}: {condition}")
                search = search_method(attr.within(condition))
                return search
        elif condition == "has_any_value" and search_method_name == "where_not":
            # Special case for has_any_value in negative conditions
            logger.debug(f"Excluding assets where {attr_name} has any value")
            search = search_method(attr.has_any_value())
            return search
        else:
            # Default to equality operator
            logger.debug(
                f"Applying {search_method_name} equality condition {attr_name}={condition}"
            )
            search = search_method(attr.eq(condition))
            return search
```

## modelcontextprotocol/.dockerignore

```text
# Environment files
.env
.env.*

# Python virtual environments
.venv
venv/

# Python cache and compiled files
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
*.so

# IDE and editor files
.cursor/
.vscode/
.vscodeignore
.idea/
.DS_Store

# Git
.git/
.gitignore

# Python development and testing
.pytest_cache/
.ruff_cache/
.mypy_cache/
.coverage
.tox/
.nox/

# Python build artifacts
*.egg-info/
dist/
build/


# Development configuration
.pre-commit-config.yaml
```

## modelcontextprotocol/.python-version

```text
3.11
```

## modelcontextprotocol/settings.py

```python
"""Configuration settings for the application."""

from typing import Optional
from pydantic_settings import BaseSettings
from version import __version__ as MCP_VERSION


class Settings(BaseSettings):
    """Application settings loaded from environment variables or .env file."""

    ATLAN_BASE_URL: str
    ATLAN_API_KEY: str
    ATLAN_AGENT_ID: str = "NA"
    ATLAN_AGENT: str = "atlan-mcp"
    ATLAN_MCP_USER_AGENT: str = f"Atlan MCP Server {MCP_VERSION}"
    MCP_TRANSPORT: str = "stdio"
    MCP_HOST: str = "0.0.0.0"
    MCP_PORT: int = 8000
    MCP_PATH: str = "/"

    @property
    def headers(self) -> dict:
        """Get the headers for API requests."""
        return {
            "User-Agent": self.ATLAN_MCP_USER_AGENT,
            "X-Atlan-Agent": self.ATLAN_AGENT,
            "X-Atlan-Agent-Id": self.ATLAN_AGENT_ID,
            "X-Atlan-Client-Origin": self.ATLAN_AGENT,
        }

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        extra = "allow"
        # Allow case-insensitive environment variables
        case_sensitive = False


_settings: Optional[Settings] = None


def get_settings() -> Settings:
    """
    Get the singleton Settings instance.
    Loads settings once from environment/file and reuses the instance.

    Returns:
        Settings: The singleton settings instance
    """
    global _settings
    if _settings is None:
        _settings = Settings()
    return _settings
```

## modelcontextprotocol/middleware.py

```python
"""
Tool restriction middleware for FastMCP to control tool access.

This middleware restricts access to specified tools based on configuration.
Tools can be restricted globally by providing a list during initialization.
"""

from typing import List, Set, Optional
from fastmcp.server.middleware import Middleware, MiddlewareContext
from fastmcp.exceptions import ToolError
import logging

logger = logging.getLogger(__name__)


class ToolRestrictionMiddleware(Middleware):
    """
    Middleware to restrict tool access based on configuration.

    Allows specifying which tools should be restricted during initialization.
    Restricted tools will be hidden from the tools list and blocked from execution.
    """

    def __init__(self, restricted_tools: Optional[List[str]] = None):
        """
        Initialize the Tool Restriction Middleware.

        Args:
            restricted_tools: List of tool names to restrict. If None, no tools are restricted.
        """
        self.restricted_tools: Set[str] = set(restricted_tools or [])
        self._log_initialization()

    def _log_initialization(self) -> None:
        """Log middleware initialization details."""
        logger.info(
            f"Tool Restriction Middleware initialized with {len(self.restricted_tools)} restricted tools",
            restricted_tools=list(self.restricted_tools),
        )

    def _is_tool_restricted(self, tool_name: str) -> bool:
        """
        Check if a tool is restricted.

        Args:
            tool_name: Name of the tool being called.

        Returns:
            True if the tool is restricted, False otherwise.
        """
        is_restricted = tool_name in self.restricted_tools

        if is_restricted:
            logger.info(f"Tool {tool_name} is restricted", tool=tool_name)

        return is_restricted

    def _get_error_message(self, tool_name: str) -> str:
        """
        Get appropriate error message for a restricted tool.

        Args:
            tool_name: Name of the restricted tool.

        Returns:
            Error message string.
        """
        return f"Tool '{tool_name}' is not available due to access restrictions"

    async def on_call_tool(self, context: MiddlewareContext, call_next):
        """
        Hook called when a tool is being executed.

        Checks if the tool is restricted and either allows execution or raises an error.

        Args:
            context: The middleware context containing request information.
            call_next: Function to call the next middleware/handler in the chain.

        Returns:
            The result from the next handler if allowed.

        Raises:
            ToolError: If the tool is restricted.
        """
        tool_name = context.message.name

        try:
            # Check if tool is restricted
            if self._is_tool_restricted(tool_name):
                error_message = self._get_error_message(tool_name)

                logger.warning(
                    f"Tool access denied: {tool_name}",
                    tool=tool_name,
                    reason=error_message,
                )

                raise ToolError(error_message)

            # Tool is allowed, proceed with execution
            logger.debug(f"Tool access granted: {tool_name}", tool=tool_name)

            return await call_next(context)

        except ToolError:
            # Re-raise ToolError as-is
            raise
        except Exception as e:
            # Handle unexpected errors
            logger.error(
                f"Error in tool restriction middleware: {str(e)}",
                tool=tool_name,
                exc_info=True,
            )
            # Re-raise the original exception
            raise

    async def on_list_tools(self, context: MiddlewareContext, call_next):
        """
        Hook called when listing available tools.

        Filters the tool list to hide restricted tools.

        Args:
            context: The middleware context.
            call_next: Function to call the next handler.

        Returns:
            Filtered list of tools.
        """
        # Get the full list of tools
        all_tools = await call_next(context)

        try:
            # If no tools are restricted, return all tools
            if not self.restricted_tools:
                return all_tools

            # Filter out restricted tools
            filtered_tools = [
                tool for tool in all_tools if tool.name not in self.restricted_tools
            ]

            logger.debug(
                "Filtered tool list",
                total_tools=len(all_tools),
                filtered_tools=len(filtered_tools),
                restricted_tools=list(self.restricted_tools),
            )

            return filtered_tools

        except Exception as e:
            logger.error(
                f"Error filtering tool list: {str(e)}",
                exc_info=True,
            )
            # On error, return the original list to avoid breaking functionality
            return all_tools
```

## Statistics

- Total Files: 41
- Total Characters: 415266
- Total Tokens: 0
